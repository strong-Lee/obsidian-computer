作为一名资深 PHP 开发者，你一定对 PHP 的数组（Array）引以为豪。PHP 的数组是计算机工程史上的一个奇迹：它用一种结构同时实现了 List、Map、Queue、Stack。但这种“万能”是有代价的。

CPython 的字典（Dict）则走了一条完全不同的路：**极致的哈希性能优化**。为了达到这个目标，它甚至牺牲了部分灵活性。

我们要深入 CPython 的源码核心文件 Objects/dictobject.c。请系好安全带，我们要把这辆跑车拆开来看引擎了。

### 模块 1: 内存模型与演变 (Memory & Evolution)

#### 1.1 PHP 的“万能”代价 vs Python 的“极致”追求

**场景/问题**：我们需要存储一组键值对，并且希望记住它们的插入顺序。

**PHP 的设计决策 (Zend Array)**：  
PHP 的 HashTable 为了支持“有序性”和“万能性”，采用了一种非常“重”的结构 —— **拉链法 (Chaining) + 双向链表**。

- **Bucket 结构**：每个元素不仅仅存 Key/Value，还必须存 next 指针（解决哈希冲突）以及 pListNext 和 pListLast 指针（维护插入顺序）。
- **内存视角**：这就好比你住酒店。在 PHP 的酒店里，每个房间（Bucket）里除了放行李，还挂着两根绳子，一根牵着上一个入住的人，一根牵着下一个入住的人。
- **代价**：巨大的内存开销（每个元素多出 2-3 个指针大小的 Meta Data）和较差的 CPU 缓存命中率（因为链表在内存中是跳跃的）。
    
**Python 的设计决策 (CPython Dict)**：  
Python 早期（3.6 之前）也是无序的，浪费大量内存。但从 Python 3.6 开始（正式确立于 3.7），引入了革命性的 **Compact Dict (紧凑字典)**。

**推导链**：

1. **痛点**：旧版 Python 字典是一个巨大的 PyDictEntry 数组。如果我申请了 8 个位置，只用了 1 个，那 7 个 Entry 结构体（包含 hash, key指针, value指针，约 24 字节/个）全是空的，浪费了 7 * 24 字节。
2. **思考**：为什么要把“空位”做得那么大？“空位”只需要告诉我“这里没人”就行了，不需要占一个结构体的位置。
3. **决策**：**将“索引”和“数据”分离**。
    - 搞一个极小的数组只存索引（Indices），它只负责占位。
    - 搞一个紧凑的数组只存数据（Entries），它只负责按顺序追加。
#### 1.2 紧凑字典的解剖 (Indices vs Entries)

请在脑海中构建这样两块内存区域：

**A. dk_indices (索引表/哈希表)**  
这是一个单纯的整数数组。它的长度就是哈希表的大小（如 8）。

- 它的作用仅仅是：通过 Hash % Size 算出的位置，告诉我们“数据在 Entries 数组的第几个位置”。
- 因为只是存下标，如果 Entries 少于 128 个，这里每个格子只需要 **1 个字节 (int8)**！
- 内存里长这样：`[-1, 0, -1, 1, 2, -1, -1, -1]`
    
    - -1 (或 DKIX_EMPTY) 代表空。
    - 0 代表数据在 Entries[0]。
    - 2 代表数据在 Entries[2]。        

**B. dk_entries (实体表/数据表)**  
这是一个 PyDictKeyEntry 结构体的数组。

- **关键点**：它是 **Append-only**（只追加）的。
- 你插入第一个元素 `('a', 1)`，它就放在 `Entries[0]`。
- 你插入第二个元素 `('b', 2)`，它就放在 `Entries[1]`。
- **天然有序**：因为你是按顺序追加的，所以遍历这个数组，就是按插入顺序遍历！无需像 PHP 那样维护昂贵的双向链表。

**对比图解**：

> 假设插入：`d['a']=1, d['b']=2` (假设 `hash('a')%8=1, hash('b')%8=4`)

**Python 3.6+ (Compact)**:

```text
Indices (int8 array, size 8):
Index:  0   1   2   3   4   5   6   7
Value: -1   0  -1  -1   1  -1  -1  -1
            |           |
            v           v
Entries (Dense Array):
[0]: {hash_a, ptr_key_a, ptr_val_1}  <-- 紧挨着
[1]: {hash_b, ptr_key_b, ptr_val_2}  <-- 内存连续！
```

**PHP (Zend Array - Simplified)**:

```text
Bucket 1: [KeyA, Val1, Next->Bucket4, Prev->NULL]
... (内存可能不连续) ...
Bucket 4: [KeyB, Val2, Next->NULL, Prev->Bucket1]
```

**底层原理总结**：  
Python 的这种设计，使得内存占用减少了 20%-30%（因为大量的空位从 24 字节/个变成了 1 字节/个）。同时，遍历字典变成了对 dk_entries 的线性内存扫描 (ptr++)，这对 CPU 的预取器（Prefetcher）极其友好，速度飞快。

---

### 模块 2: 哈希冲突与探查机制 (Collision & Probing)

#### 2.1 冲突解决：PHP 的温柔 vs Python 的暴力

**场景/问题**：当 hash(key1) 和 hash(key2) 算出来的数组下标（Bucket Index）一样时，怎么办？

**PHP 的设计决策：拉链法 (Chaining)**

- **做法**：每个 Bucket 维护一个链表。冲突了？没关系，挂在当前元素的屁股后面（zval.next）。
    
- **权衡**：
    
    - 优点：实现简单，对负载因子不敏感（满了也不怕，链表变长而已）。
    - 缺点：**缓存杀手**。链表节点在堆内存中是随机分布的。CPU 访问 Bucket->next 时，大概率会发生 Cache Miss（因为数据不在当前的 CPU 缓存行中），需要去慢速的 RAM 里捞数据。
        

**Python 的设计决策：开放寻址法 (Open Addressing)**

- **做法**：如果位置被占了，我就去**找下一个空位**。我不挂链表，所有人都必须住在 indices 这个数组里。
    
- **推导链**：
    
    1. **观察**：现代 CPU 的运算速度极快，瓶颈通常在于内存 I/O。
    2. **缓存行 (Cache Line)**：CPU 从内存读取数据时，不是读 1 个字节，而是一次读一块（通常 64 字节）。
    3. **决策**：如果我使用开放寻址，当我检查 `Indices[i]` 发现有人时，我去检查 `Indices[i+1]`。因为它们在内存中是连续的，`Indices[i+1]` 很可能已经在 CPU 缓存里了！这比 PHP 去内存的另一个角落找 next 指针要快几十倍。
        

#### 2.2 扰动策略 (Perturbation) —— 不仅仅是 +1

**问题**：如果冲突了，下一个位置找谁？  
最简单的做法是 index = (index + 1) % size (线性探测)。  
但这有个致命问题：**群集效应 (Clustering)**。如果 hash 算法不好，导致一堆 Key 都挤在 index 10 附近，那么每次插入都要把这堆 Key 遍历一遍才能找到空位，性能退化成 O(N)。

**`CPython` 的黑魔法**：  
Python 使用了一种复杂的伪随机探测序列。在 `dictobject.c` 的 `lookdict` 函数中：

```c
// C 伪代码
perturb = hash;
while (slot is occupied) {
    // 每次迭代，i 不是简单的 +1
    // 而是由当前的 i 和 perturb 共同决定
    i = (i * 5 + perturb + 1) & mask;
    
    // perturb 每次右移 5 位
    perturb >>= 5; 
}
```

**设计深度解析**：

1. **i * 5 + 1**：这保证了能够遍历完所有的槽位（数论原理），让探测序列跳跃分布，避免局部群集。
2. **+ perturb**：这是神来之笔。
    - 初始 perturb 是 Key 的原始 Hash 值。
    - 这意味着：**Hash 值的高位比特（High Bits）参与了探测序列的决策**。
    - 如果不加 perturb，只有 Hash 值的低位（对应数组大小掩码）参与运算。例如，如果由 8 个槽，只有 Hash 的最后 3 位有效。
    - 引入 perturb 后，即使两个数的低 3 位一样（碰撞），只要它们的高位不一样，它们的探测路径就会**迅速分叉**，去完全不同的地方找空位。
    - 这极大地增强了对“构造性哈希冲突攻击”的防御力，也解决了某些特定模式数据的冲突问题（例如一系列地址对齐的指针做 Key）。
        

---

### 模块 3: 动态扩容与 Rehash (Resizing & Rehash)

#### 3.1 负载因子：Python 为什么这么“沉不住气”？

**场景/问题**：哈希表什么时候该扩容？

**PHP 的策略**：  
PHP 往往比较宽容，通常等到 Bucket 数量不够或者达到特定比例才扩容。因为它是拉链法，即使满了，挂链表也能活，性能下降是线性的。

**Python 的策略**：  
CPython 严格规定：**负载因子 (Load Factor) 只要超过 2/3 (66%)，必须立即扩容**。

**推导链**：

1. **回顾**：Python 用的是**开放寻址法**。
    
2. **数学事实**：在开放寻址法中，当表变满时，碰撞概率呈指数级上升。
    
    - 如果表空 90%，几乎是一次命中。
    - 如果表满 80%，查找一个空位可能需要探测 5-10 次。
    - 如果满 90%，可能要探测几十次。性能会从 O(1) 暴跌。
        
3. **决策**：为了保住 O(1) 的金字招牌，必须保证表里永远有 **1/3** 的位置是空的，作为“缓冲地带”。
    

#### 3.2 Rehash 的昂贵代价

**问题**：List 扩容只是 realloc（可能涉及 memcpy），为什么 Dict 扩容被视为“性能杀手”？

**内存视角**：  
List 是连续数组，扩容是从 `[A, B, C]` 变成 `[A, B, C, _, _]`。元素的相对位置（Index 0, 1, 2）是不变的。

Dict 扩容是从 Size 8 变成 Size 16。

- Key "A" 的 Hash 是 9。
- 旧表位置：9 % 8 = 1。
- 新表位置：9 % 16 = 9。
- **位置变了！**

**残酷的现实**：  
当 Python 字典扩容时，它必须：

1. 申请一块新的、更大的内存（dk_indices 和 dk_entries）。
2. **遍历**旧表中的每一个 Entry。
3. **Re-insert**：拿着 Entry 的 Hash 值，在新表中**重新走一遍探查流程**（Collision Probing），找到新的落脚点。
4. 这不仅是内存拷贝，这是大量的 CPU 计算和内存随机访问。
    

**性能陷阱演示**：

```python
d = {}
# 假设这里触发了扩容
for i in range(1000000): 
    # 在 i = 某几个 2/3 阈值点时（如 43k, 87k...）
    # 这一行代码的耗时会突然飙升 10-100 倍
    d[i] = i
```

这就是为什么在第一部分 Tips 中，我强烈建议使用 dict(zip(...)) 或 update()，因为 CPython 可以在这些操作中**预先计算**最终大小，直接申请一次大内存，避免中间触发多次 Rehash。

---

### 模块 4: Key 的限制与底层协议 (Key Constraints)

#### 4.1 为什么 List 不能做 Key？

**场景/问题**：PHP 的 Key 只能是 Int/String，Python 允许 Object。但为什么 {[1]: 1} 会报错 TypeError: unhashable type: 'list'？

从 **C 源码 (Objects/listobject.c)** 视角来看：  
任何 Python 对象在 C 层都是 PyObject，它有一个类型对象 PyTypeObject。这个类型结构体里有一个函数指针成员叫 tp_hash。

- 对于 Tuple：`tp_hash` 指向 `tuplehash` 函数（会根据内容计算 Hash）。
- 对于 List：`tp_hash` 被显式设置为 **`PyObject_HashNotImplemented`** (即 NULL)。
    

**设计决策推导**：

1. **假设**：我们允许 List 做 Key。
2. **过程**：
    - `L = [1]`
    - `d[L] = "value"`。Python 计算 hash(L) 得到 123，放入 Slot 123。
    - L.append(2)。List 变了，内容变成了 `[1, 2]`。
    - 用户想读取 `d[L]`。Python 再次计算 `hash(L)`，因为内容变了，Hash 值变成了 456。
    - Python 去 Slot 456 找数据 —— **找不到！**
3. **后果**：这个 Key-Value 对虽然还在内存里（Slot 123），但永远“失联”了。这破坏了哈希表的基本契约。
4. **结论**：**可变对象 (Mutable) 绝不可哈希**。
    

#### 4.2 为什么 1 和 1.0 是同一个 Key？

**现象**：`d = {1: "a"}; d[1.0] = "b"; print(d)` 结果是 `{1: "b"}`。

**底层协议 (PyDict_GetItem)**：  
Python 字典在查找 Key 时，遵循严格的**两步验证**：

1. **Step 1: 身份与哈希检查 (Fast Path)**
    - 先看指针地址 (key_ptr == query_ptr)？如果是同一个对象，直接命中。
    - 如果不等，计算 Hash。如果 hash(key) != hash(query)，直接排除。
    - **关键点**：Python 的 float 哈希算法经过精心设计，使得 hash(1.0) == 1，而 hash(1) == 1。第一关过了。
        
2. **Step 2: 值相等性检查 (Slow Path)**
    - 如果 Hash 一样，Python 调用 PyObject_RichCompareBool (即 == 运算)。
    - 1 == 1.0 在 Python 中为 True。
    - **判定**：这是同一个 Key！
        

**内存视角的决策**：  
虽然 1 (PyLongObject) 和 1.0 (PyFloatObject) 在内存中是两个完全不同的结构体，占用的字节数都不一样。但在字典的逻辑层，它们被视为同一个“索引”。  
CPython 这样设计是为了符合数学直觉（数字 1 就是 1.0），但这要求开发者必须理解：**字典的 Key 唯一性取决于 hash 和 eq，而不是内存地址。**

---

### 总结：架构师的视角

通过这四个模块，你应该能看到 Python 字典设计的核心哲学：

1. **空间换时间，再用时间换空间**：
    - 早期的 Python 为了快，用了稀疏数组（浪费空间）。
    - 现代 Python (Compact Dict) 通过引入中间层 (Indices)，既保留了 O(1) 的快，又把空间省回来了，甚至还送了“有序性”这个大礼包。
        
2. **CPU Cache 是上帝**：
    - 放弃拉链法，选择开放寻址法，就是为了讨好 CPU 的 L1/L2 缓存。这是高性能系统的通用设计法则。
        
3. **严防死守**：
    - Rehash 的 2/3 阈值、Perturbation 的扰动算法，都是为了防止最坏情况（Worst Case）的发生，保证系统的稳定性。
        

这就是为什么 Python 的字典如此强大。它不是简单的“数组”，它是算法与现代硬件体系结构结合的艺术品。