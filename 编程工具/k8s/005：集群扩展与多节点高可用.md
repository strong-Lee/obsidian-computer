单个MicroK8s实例只能运行在单个节点上。为了实现高可用和水平扩展，我们需要将MicroK8s集群扩展到多个节点。

### 5.1 多节点集群的优势

> [!SUCCESS] 优势
> 
> - **高可用性 (High Availability, HA)：** 如果一个节点发生故障，K8s可以将Pod重新调度到其他健康的节点上，确保应用持续运行。
>     
> - **水平扩展 (Horizontal Scaling)：** 当单个节点的资源不足以支撑所有应用时，可以添加更多节点来增加集群的总资源，从而承载更多的Pod和更高的负载。
>     
> - **资源隔离：** 不同的应用可以运行在不同的节点上，避免相互干扰。
>     

### 5.2 添加工作节点

我们将使用Multipass再创建一个虚拟机作为工作节点。

**操作步骤：**

1. **创建新的虚拟机作为工作节点：**  
    在宿主机Terminal中执行：
    
    ```bash
    multipass launch -n node-1 -c 2 -m 4GB -d 40GB
    ```
    
    > [!INFO] 目的  
    > 提供额外的计算资源，作为K8s集群的成员。资源分配可以根据实际需求调整。
    
2. **在主节点上生成加入命令：**  
    进入 microk8s-vm 主节点Shell：
    
    
    ```bash
    multipass shell microk8s-vm
    microk8s add-node
    ```
    
    > [!INFO] 目的与原理
    > 
    > - **microk8s add-node：** 这个命令会生成一个包含IP地址、端口和一次性令牌的命令字符串，用于新节点加入集群。
    >     
    > - **原理：** 它实际上是在主节点上生成一个Join Token，并准备API服务器接受新的节点连接。
    >     
    
3. **在新工作节点上执行加入命令：**  
    打开一个新的Terminal窗口，进入 node-1 虚拟机的Shell：
    
    ```bash
    multipass shell node-1
    # 粘贴并执行从主节点获取的join命令，例如：
    microk8s join 192.168.64.4:25000/f5e16681775499b4f21a974a9a753ccb/ff09c81e4146 --worker
    ```
    
    > [!INFO] 命令解释与原理
    > 
    > - **--worker：** 指定这个节点将作为工作节点加入集群，而不是作为控制平面节点（如果您想构建高可用的控制平面，需要更多节点和--cluster选项）。
    >     
    > - **原理：** 工作节点通过这个命令连接到主节点的Kube-APIServer，并向主节点注册自己。Kubelet（工作节点上的代理）会开始运行，并与控制平面通信。
    >     
    
4. **在主节点上验证集群状态：**  
    回到 microk8s-vm 主节点Shell：
    
    ```bash
    kubectl get nodes -o wide
    ```
    
    > [!INFO] 验证与高可用
    > 
    > - **验证：** 您应该看到 microk8s-vm 和 node-1 两个节点，且 STATUS 都为 Ready。
    >     
    > - **高可用：** 现在，如果 microk8s-vm 上的Pod因为资源不足或节点故障而无法运行时，K8s调度器有机会将其调度到 node-1 上。
    >     
    

### 5.3 Pod调度与节点亲和性 (高级)

有了多节点集群后，K8s调度器会根据资源的请求量、节点负载、亲和性/反亲和性规则等来决定将Pod调度到哪个节点。

- **资源请求/限制：** 正如之前所说，Pod的CPU/内存 requests 是调度器的主要依据。
    
- **节点亲和性 (Node Affinity)：**
    
    > [!INFO] 概念与场景
    > 
    > - **概念：** 强制或倾向于将Pod调度到具有特定标签的节点上。
    >     
    > - **场景：** 例如，将高性能计算Pod调度到带有GPU的节点，或者将数据库Pod调度到带有SSD存储的节点。
    >     
    > - **示例：**
    >     
    >     ```yaml
    >     spec:
    >       nodeSelector: # 简单形式
    >         disktype: ssd
    >       affinity: # 复杂形式
    >         nodeAffinity:
    >           requiredDuringSchedulingIgnoredDuringExecution: # 强制要求
    >             nodeSelectorTerms:
    >             - matchExpressions:
    >               - key: disktype
    >                 operator: In
    >                 values:
    >                 - ssd
    >     ```
    
- **Pod亲和性/反亲和性 (Pod Affinity/Anti-affinity)：**
    
    > [!INFO] 概念与场景
    > 
    > - **概念：** 强制或倾向于将某些Pod调度到与特定Pod相同/不同节点上。
    >     
    > - **亲和性优点：** 将相关服务部署在同一节点可以减少网络延迟。
    >     
    > - **反亲和性优点：** 将应用的多个副本分散到不同节点，提高高可用性（避免单点故障）。
    >     
    > - **场景：**
    >     
    >     - **亲和性：** Web应用和其缓存服务在同一节点。
    >         
    >     - **反亲和性：** 数据库的主从复制实例分散在不同节点，确保一个节点挂掉不影响整个数据库服务。
    >         
    > - **示例：**
    >     
    >     ```yaml
    >     spec:
    >       affinity:
    >         podAntiAffinity: # 反亲和性示例
    >           requiredDuringSchedulingIgnoredDuringExecution: # 强制不调度到同一节点
    >             - labelSelector:
    >                 matchLabels:
    >                   app: iot-api
    >               topologyKey: kubernetes.io/hostname # 在不同主机名（即不同节点）上
    >     ```
    >     
    > - **原理：** 调度器会根据这些规则计算每个节点的“得分”，然后选择得分最高的节点进行调度。
    >