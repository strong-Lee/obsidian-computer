理解K8s的底层原理，才能在面对高并发、性能瓶颈时做出正确的判断和优化。

### 6.1 单服务器极限性能与资源管理

即使是单节点K8s，理解资源管理也是基础。

1. **资源请求与限制 (Requests & Limits) - 重申与深挖：**
    
    > [!INFO] 原理与优化
    > 
    > - **原理：** K8s利用Linux的**cgroups (Control Groups)** 来实现资源的隔离和限制。
    >     
    >     - **CPU Limits:** 当Pod的CPU使用量达到Limit时，会被**限流**，而不是被Kill。这意味着CPU使用会被节流，性能会下降，但服务不会中断。
    >         
    >     - **Memory Limits:** 当Pod的内存使用量达到Limit时，**会被Killed (OOMKill)**。这是因为内存资源是不可压缩的，一旦用尽，系统会选择终止进程以维持稳定性。
    >         
    > - **优化：**
    >     
    >     - **精确估算：** 监控应用在典型负载下的资源使用情况，合理设置requests和limits，不要过高也不要过低。
    >         
    >     - **设置Limits略高于Requests：** 允许Pod在高峰期爆发性地使用更多资源，但不会超出节点承载能力。
    >         
    >     - **避免Over-provisioning：** 不要给Pod分配过多的资源，这会浪费节点资源，降低集群整体利用率。
    >         
    >     - **避免Under-provisioning：** 资源过低会导致频繁的OOMKill或CPU限流，影响服务可用性和性能。
    >         
    
2. **Pod QoS (Quality of Service) 等级：**  
    K8s根据Pod的requests和limits设置，将其分为三类QoS等级：
    
    > [!INFO] QoS等级与原理
    > 
    > - **Guaranteed (保证型)：** 所有容器的CPU和内存的requests和limits都相等且都已设置。最高优先级，资源最稳定。
    >     
    > - **Burstable (突发型)：** 至少有一个容器的requests小于limits，或者只设置了requests。中等优先级，在资源充足时可突发使用更多资源。
    >     
    > - **BestEffort (尽力而为型)：** 没有设置任何requests或limits。最低优先级，资源最不稳定，最容易被驱逐。
    >     
    > - **原理：** 在资源紧张时，K8s会根据QoS等级决定驱逐哪些Pod。BestEffort最先被驱逐，Guaranteed最后。
    >     
    > - **优化：** 核心业务应用应尽量达到Guaranteed或Burstable级别，确保其资源稳定性。
    >     
    
3. **垂直扩展 (Vertical Pod Autoscaler, VPA) - 了解概念：**
    
    > [!INFO] 概念与优缺点
    > 
    > - **概念：** VPA根据历史资源使用情况，自动调整Pod的CPU和内存requests和limits。
    >     
    > - **优点：** 自动化资源管理，减少手动调优的工作量，提高资源利用率。
    >     
    > - **缺点：** 调整Pod资源通常需要重启Pod，可能导致短暂的服务中断。目前MicroK8s默认没有内置VPA插件。
    >     
    > - **与HPA的区别：** [[#水平 Pod 自动伸缩 (Horizontal Pod Autoscaler, HPA)|HPA]]是水平扩展Pod数量，VPA是垂直调整单个Pod的资源。
    >     
    

### 6.2 高并发、多服务器场景处理

高并发意味着大量的并发请求，多服务器是应对这种挑战的关键。

1. **水平 Pod 自动伸缩 (Horizontal Pod Autoscaler, HPA)：**
    
    > [!INFO] 原理与目的
    > 
    > - **原理：** HPA会根据Pod的CPU利用率、内存利用率或自定义指标（如每秒请求数QPS）自动增加或减少Deployment/StatefulSet的Pod副本数量。
    >     
    > - **如何工作：** HPA控制器持续监控Pod的指标，当指标超过或低于预设阈值时，HPA会触发Deployment/StatefulSet控制器来增加或减少Pod数量。
    >     
    > - **为什么这么做？**
    >     
    >     - **弹性扩展：** 自动适应流量波动，避免在高峰期出现性能瓶颈，在低峰期节省资源。
    >         
    >     - **成本效益：** 按需分配资源，避免过度预留。
    >         
    >     - **高可用：** 更多副本意味着更好的抗故障能力。
    >         
    > - **前提：** [[Metrics Server]] 插件必须启用，以便HPA能够获取Pod的资源指标。
    >     
    > - **示例：**
    >     
    >     ```bash
    >     # 基于CPU利用率自动伸缩，当CPU利用率超过50%时，自动增加Pod数量
    >     # 最小1个副本，最大10个副本
    >     kubectl autoscale deployment iot-api-service --cpu-percent=50 --min=1 --max=10 -n aila
    >     ```
    
2. **负载均衡 (Load Balancing) - 深入：**
    
    - **Service (内部LB)：**
        
        > [!INFO] 原理与工作模式
        > 
        > - **原理：** K8s [[Service]] 通过 kube-proxy 组件实现内部的负载均衡。kube-proxy 在每个节点上运行，它会监听K8s Service和Endpoint的变化，并根据Service的配置更新节点的 iptables 规则（或IPVS规则），将发往Service ClusterIP的流量转发到后端Pod。
        >     
        > - **工作模式：** 默认为轮询（Round Robin）或随机分发。
        >     
        
    - **Ingress (外部LB)：**
        
        > [!INFO] 原理与工作模式
        > 
        > - **原理：** [[Nginx Ingress Controller]]（如Nginx Ingress）作为反向代理，接收外部流量，并根据Ingress规则将其转发到内部Service。它通常实现更高级的负载均衡算法（如加权轮询、最少连接）和健康检查。
        >     
        > - **如何工作：** Ingress Controller会监听Ingress资源的变化，并动态生成其代理（如Nginx）的配置文件，实现流量路由。
        >     
        > - **多服务器协作：** Ingress Controller通常也以多副本部署，它们都可以监听外部请求，并将流量分发到集群内部的不同节点上的Pod，形成一个分布式负载均衡体系。
        >     
        
3. **DNS 服务发现：**
    
    > [!INFO] 目的与原理
    > 
    > - **原理：** [[CoreDNS]] 作为集群的DNS服务器，为K8s中的[[Service]]创建DNS记录。Pod在尝试连接服务时，会通过DNS查询服务名（如mysql-service.database），CoreDNS将解析为对应的ClusterIP或Pod IP（对于Headless Service）。
    >     
    > - **为什么重要？** 实现了服务的解耦和动态发现，应用无需知道后端服务的实际IP，只需知道其逻辑名称，大大简化了配置和管理。
    >     
    
4. **无状态应用 (Stateless Applications) 与有状态应用 (Stateful Applications)：**
    
    - **无状态应用：** 每次请求不依赖于之前的请求或任何本地存储的数据。例如，简单的Web服务器、API网关。
        
        > [!INFO] 如何处理高并发  
        > 易于水平扩展。只需增加[[Deployment]]的副本数量，每个副本都是独立的，可以通过负载均衡器均匀分发请求。
        
    - **有状态应用：** 需要维护状态，例如数据库、消息队列。
        
        > [!INFO] 如何处理高并发  
        > 挑战性更高。
        > 
        > - **数据库：** 通常采用主从复制、分库分表、读写分离等方案。在K8s中，这意味着部署多个数据库实例（使用[[StatefulSet]]），并将读请求路由到只读副本，写请求路由到主副本。
        >     
        > - **分布式存储：** 对于需要共享文件存储的应用，可以使用NFS、Ceph (Rook) 等分布式文件系统，确保数据在不同节点间可见。
        >     
        > - **缓存：** 使用Redis、Memcached等分布式缓存，减轻数据库压力。
        >     
        
        > [!INFO] 分布式数据库原理 (CAP 定理 - 简单了解)
        > 
        > - **一致性 (Consistency)：** 所有节点在同一时间看到的数据都是一致的。
        >     
        > - **可用性 (Availability)：** 所有请求都能收到响应（非错误响应），即使部分节点故障。
        >     
        > - **分区容错性 (Partition tolerance)：** 尽管网络分区（节点间通信故障），系统仍能继续运行。
        >     
        > - **分布式系统只能同时满足其中两个。** K8s集群本身更倾向于AP (可用性+分区容错)，通过复制和重新调度来保证服务可用性。选择数据库时，需要权衡C、A、P。
        >     
        

### 6.3 监控与日志

性能优化离不开数据。

- **监控：**
    
    > [!INFO] 工具与目的
    > 
    > - **[[Metrics Server]]：** 提供基本的CPU/内存指标。
    >     
    > - **Prometheus + Grafana：** 强大的开源监控解决方案。Prometheus负责数据采集和存储，Grafana负责可视化。MicroK8s可以集成它们。
    >     
    > - **目的：** 实时了解集群、节点、Pod的资源使用情况、网络流量、应用响应时间等，及时发现瓶颈。
    >     
    
- **日志：**
    
    > [!INFO] 工具与目的
    > 
    > - **kubectl logs：** 查看单个Pod的日志。
    >     
    > - **集中式日志系统 (ELK Stack / Loki)：**
    >     
    >     - **ELK (Elasticsearch, Logstash, Kibana)：** 将所有Pod的日志收集到中心存储，方便搜索、分析和可视化。
    >         
    >     - **Loki + Grafana：** 类似Prometheus的日志系统，但专注于日志索引和查询，更轻量。
    >         
    > - **目的：** 追踪应用行为、调试问题、审计操作。在分布式系统中，统一的日志系统至关重要。
    >